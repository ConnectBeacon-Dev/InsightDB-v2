#!/usr/bin/env python3
"""
Test Cases for Non-English CSV Parsing

This module tests the IntegratedSearchFileCreator's ability to handle:
- Various character encodings (UTF-8, UTF-8-BOM, CP1252, Latin-1, etc.)
- International characters (Chinese, Japanese, Arabic, Hindi, Cyrillic, etc.)
- Mixed encoding scenarios
- Edge cases with special characters and symbols
"""

import unittest
import pandas as pd
import tempfile
import json
import shutil
from pathlib import Path
from typing import Dict, List, Any
import io
import sys
import os

# Add the parent directory to Python path to import the main module
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

try:
    # Import the class we're testing
    from create_integrated_search_file import IntegratedSearchFileCreator
except ImportError as e:
    print(f"Warning: Could not import IntegratedSearchFileCreator: {e}")
    print("Running in standalone mode with manual tests only")
    IntegratedSearchFileCreator = None


class TestNonEnglishCSVParsing(unittest.TestCase):
    """Test suite for non-English CSV parsing capabilities."""
    
    def setUp(self):
        """Set up test workspace."""
        if IntegratedSearchFileCreator is None:
            self.skipTest("IntegratedSearchFileCreator not available")
            
        self.temp_dir = tempfile.mkdtemp()
        self.workspace = Path(self.temp_dir)
        
        # Create required directory structure
        (self.workspace / "input_data" / "data_flat_file").mkdir(parents=True)
        (self.workspace / "input_data" / "table_relations").mkdir(parents=True)
        (self.workspace / "processed_data_store" / "company_mapped_store").mkdir(parents=True)
        
        # Create sample relations.json file
        relations = [
            {
                "from_table": "CompanyMaster",
                "from_column": "Country_Fk_id",
                "to_table": "CountryMaster",
                "to_column": "Id"
            },
            {
                "from_table": "CompanyMaster",
                "from_column": "CompanyScale_Fk_Id",
                "to_table": "ScaleMaster",
                "to_column": "Id"
            }
        ]
        
        relations_path = self.workspace / "input_data" / "table_relations" / "relations.json"
        with open(relations_path, 'w', encoding='utf-8') as f:
            json.dump(relations, f, ensure_ascii=False)
    
    def tearDown(self):
        """Clean up test workspace."""
        if hasattr(self, 'temp_dir'):
            shutil.rmtree(self.temp_dir)
    
    def create_csv_with_encoding(self, filepath: Path, data: Dict, encoding: str, 
                                include_bom: bool = False):
        """Helper to create CSV files with specific encodings."""
        df = pd.DataFrame(data)
        
        if include_bom and encoding.lower() in ['utf-8', 'utf8']:
            encoding = 'utf-8-sig'
        
        try:
            df.to_csv(filepath, index=False, encoding=encoding)
        except UnicodeEncodeError:
            # Fallback for encodings that can't handle certain characters
            # Replace problematic characters with closest ASCII equivalent
            for col in df.columns:
                if df[col].dtype == 'object':
                    df[col] = df[col].astype(str).str.encode(encoding, errors='ignore').str.decode(encoding)
            df.to_csv(filepath, index=False, encoding=encoding)
    
    def test_utf8_chinese_characters(self):
        """Test parsing CSV with Chinese characters in UTF-8."""
        chinese_data = {
            'Id': [1, 2, 3],
            'CompanyName': ['åŒ—äº¬ç§‘æŠ€æœ‰é™å…¬å¸', 'ä¸Šæµ·åˆ¶é€ ä¼ä¸š', 'æ·±åœ³åˆ›æ–°å…¬å¸'],
            'Address': ['åŒ—äº¬å¸‚æœé˜³åŒº', 'ä¸Šæµ·å¸‚æµ¦ä¸œæ–°åŒº', 'æ·±åœ³å¸‚å—å±±åŒº'],
            'CityName': ['åŒ—äº¬', 'ä¸Šæµ·', 'æ·±åœ³'],
            'CompanyRegistrationDate': ['2020-01-15', '2019-05-20', '2021-03-10'],
            'Country_Fk_id': [1, 1, 1]
        }
        
        # Create CompanyMaster CSV
        company_csv = self.workspace / "input_data" / "data_flat_file" / "dbo.CompanyMaster.csv"
        self.create_csv_with_encoding(company_csv, chinese_data, 'utf-8')
        
        # Create CountryMaster CSV
        country_data = {
            'Id': [1],
            'CountryName': ['ä¸­å›½']
        }
        country_csv = self.workspace / "input_data" / "data_flat_file" / "dbo.CountryMaster.csv"
        self.create_csv_with_encoding(country_csv, country_data, 'utf-8')
        
        # Test parsing
        creator = IntegratedSearchFileCreator(str(self.workspace))
        success = creator.create_integrated_file()
        
        assert success, "Should successfully parse Chinese UTF-8 CSV"
        assert 'CompanyMaster' in creator.tables
        assert len(creator.tables['CompanyMaster']) == 3
        
        # Verify Chinese characters are preserved
        company_names = creator.tables['CompanyMaster']['CompanyName'].tolist()
        assert 'åŒ—äº¬ç§‘æŠ€æœ‰é™å…¬å¸' in company_names
        assert 'ä¸Šæµ·åˆ¶é€ ä¼ä¸š' in company_names
    
    def test_utf8_bom_japanese_characters(self):
        """Test parsing CSV with Japanese characters and UTF-8 BOM."""
        japanese_data = {
            'Id': [1, 2],
            'CompanyName': ['æ±äº¬ãƒ†ã‚¯ãƒãƒ­ã‚¸ãƒ¼æ ªå¼ä¼šç¤¾', 'å¤§é˜ªè£½é€ æ¥­æœ‰é™ä¼šç¤¾'],
            'Address': ['æ±äº¬éƒ½æ¸‹è°·åŒº', 'å¤§é˜ªåºœå¤§é˜ªå¸‚'],
            'CityName': ['æ±äº¬', 'å¤§é˜ª'],
            'CompanyRegistrationDate': ['2020-04-01', '2018-07-15'],
            'Country_Fk_id': [2, 2]
        }
        
        company_csv = self.workspace / "input_data" / "data_flat_file" / "dbo.CompanyMaster.csv"
        self.create_csv_with_encoding(company_csv, japanese_data, 'utf-8', include_bom=True)
        
        country_data = {
            'Id': [2],
            'CountryName': ['æ—¥æœ¬']
        }
        country_csv = self.workspace / "input_data" / "data_flat_file" / "dbo.CountryMaster.csv"
        self.create_csv_with_encoding(country_csv, country_data, 'utf-8')
        
        creator = IntegratedSearchFileCreator(str(self.workspace))
        success = creator.create_integrated_file()
        
        assert success, "Should successfully parse Japanese UTF-8-BOM CSV"
        
        # Verify Japanese characters are preserved
        company_names = creator.tables['CompanyMaster']['CompanyName'].tolist()
        assert 'æ±äº¬ãƒ†ã‚¯ãƒãƒ­ã‚¸ãƒ¼æ ªå¼ä¼šç¤¾' in company_names
    
    def test_cp1252_european_characters(self, temp_workspace, sample_relations):
        """Test parsing CSV with European characters in CP1252 encoding."""
        european_data = {
            'Id': [1, 2, 3],
            'CompanyName': ['SociÃ©tÃ© FranÃ§aise', 'Deutsche GrÃ¶ÃŸe GmbH', 'EspaÃ±ol Ã‘oÃ±o S.L.'],
            'Address': ['ParÃ­s, FranÃ§a', 'MÃ¼nchen, Deutschland', 'Madrid, EspaÃ±a'],
            'CityName': ['ParÃ­s', 'MÃ¼nchen', 'Madrid'],
            'CompanyRegistrationDate': ['2020-01-15', '2019-05-20', '2021-03-10'],
            'Country_Fk_id': [3, 4, 5]
        }
        
        company_csv = temp_workspace / "input_data" / "data_flat_file" / "dbo.CompanyMaster.csv"
        self.create_csv_with_encoding(company_csv, european_data, 'cp1252')
        
        country_data = {
            'Id': [3, 4, 5],
            'CountryName': ['France', 'Deutschland', 'EspaÃ±a']
        }
        country_csv = temp_workspace / "input_data" / "data_flat_file" / "dbo.CountryMaster.csv"
        self.create_csv_with_encoding(country_csv, country_data, 'cp1252')
        
        creator = IntegratedSearchFileCreator(str(temp_workspace))
        success = creator.create_integrated_file()
        
        assert success, "Should successfully parse CP1252 European CSV"
        
        # Verify special characters are preserved
        company_names = creator.tables['CompanyMaster']['CompanyName'].tolist()
        assert 'SociÃ©tÃ© FranÃ§aise' in company_names
        assert 'Deutsche GrÃ¶ÃŸe GmbH' in company_names
        assert 'EspaÃ±ol Ã‘oÃ±o S.L.' in company_names
    
    def test_latin1_mixed_characters(self, temp_workspace, sample_relations):
        """Test parsing CSV with mixed Latin-1 characters."""
        latin_data = {
            'Id': [1, 2],
            'CompanyName': ['CafÃ© & RÃ©sumÃ© Ltd', 'NaÃ¯ve PiÃ±ata Co'],
            'Address': ['ZÃ¼rich, Schweiz', 'SÃ£o Paulo, Brasil'],
            'CityName': ['ZÃ¼rich', 'SÃ£o Paulo'],
            'CompanyRegistrationDate': ['2020-01-15', '2019-05-20'],
            'Country_Fk_id': [6, 7]
        }
        
        company_csv = temp_workspace / "input_data" / "data_flat_file" / "dbo.CompanyMaster.csv"
        self.create_csv_with_encoding(company_csv, latin_data, 'latin-1')
        
        country_data = {
            'Id': [6, 7],
            'CountryName': ['Schweiz', 'Brasil']
        }
        country_csv = temp_workspace / "input_data" / "data_flat_file" / "dbo.CountryMaster.csv"
        self.create_csv_with_encoding(country_csv, country_data, 'latin-1')
        
        creator = IntegratedSearchFileCreator(str(temp_workspace))
        success = creator.create_integrated_file()
        
        assert success, "Should successfully parse Latin-1 CSV"
        
        # Verify accented characters are preserved
        company_names = creator.tables['CompanyMaster']['CompanyName'].tolist()
        assert 'CafÃ© & RÃ©sumÃ© Ltd' in company_names
        assert 'NaÃ¯ve PiÃ±ata Co' in company_names
    
    def test_cyrillic_characters_utf8(self, temp_workspace, sample_relations):
        """Test parsing CSV with Cyrillic (Russian) characters."""
        cyrillic_data = {
            'Id': [1, 2],
            'CompanyName': ['ĞĞĞ Ğ¢ĞµÑ…Ğ½Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ğ¸', 'ĞĞ ĞŸÑ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾'],
            'Address': ['ĞœĞ¾ÑĞºĞ²Ğ°, Ğ Ğ¾ÑÑĞ¸Ñ', 'Ğ¡Ğ°Ğ½ĞºÑ‚-ĞŸĞµÑ‚ĞµÑ€Ğ±ÑƒÑ€Ğ³, Ğ Ğ¾ÑÑĞ¸Ñ'],
            'CityName': ['ĞœĞ¾ÑĞºĞ²Ğ°', 'Ğ¡Ğ°Ğ½ĞºÑ‚-ĞŸĞµÑ‚ĞµÑ€Ğ±ÑƒÑ€Ğ³'],
            'CompanyRegistrationDate': ['2020-01-15', '2019-05-20'],
            'Country_Fk_id': [8, 8]
        }
        
        company_csv = temp_workspace / "input_data" / "data_flat_file" / "dbo.CompanyMaster.csv"
        self.create_csv_with_encoding(company_csv, cyrillic_data, 'utf-8')
        
        country_data = {
            'Id': [8],
            'CountryName': ['Ğ Ğ¾ÑÑĞ¸Ñ']
        }
        country_csv = temp_workspace / "input_data" / "data_flat_file" / "dbo.CountryMaster.csv"
        self.create_csv_with_encoding(country_csv, country_data, 'utf-8')
        
        creator = IntegratedSearchFileCreator(str(temp_workspace))
        success = creator.create_integrated_file()
        
        assert success, "Should successfully parse Cyrillic UTF-8 CSV"
        
        # Verify Cyrillic characters are preserved
        company_names = creator.tables['CompanyMaster']['CompanyName'].tolist()
        assert 'ĞĞĞ Ğ¢ĞµÑ…Ğ½Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ğ¸' in company_names
        assert 'ĞĞ ĞŸÑ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾' in company_names
    
    def test_arabic_characters_utf8(self, temp_workspace, sample_relations):
        """Test parsing CSV with Arabic characters."""
        arabic_data = {
            'Id': [1, 2],
            'CompanyName': ['Ø´Ø±ÙƒØ© Ø§Ù„ØªÙƒÙ†ÙˆÙ„ÙˆØ¬ÙŠØ§', 'Ù…Ø¤Ø³Ø³Ø© Ø§Ù„Ø¥Ù†ØªØ§Ø¬'],
            'Address': ['Ø§Ù„Ø±ÙŠØ§Ø¶ØŒ Ø§Ù„Ø³Ø¹ÙˆØ¯ÙŠØ©', 'Ø¯Ø¨ÙŠØŒ Ø§Ù„Ø¥Ù…Ø§Ø±Ø§Øª'],
            'CityName': ['Ø§Ù„Ø±ÙŠØ§Ø¶', 'Ø¯Ø¨ÙŠ'],
            'CompanyRegistrationDate': ['2020-01-15', '2019-05-20'],
            'Country_Fk_id': [9, 10]
        }
        
        company_csv = temp_workspace / "input_data" / "data_flat_file" / "dbo.CompanyMaster.csv"
        self.create_csv_with_encoding(company_csv, arabic_data, 'utf-8')
        
        country_data = {
            'Id': [9, 10],
            'CountryName': ['Ø§Ù„Ø³Ø¹ÙˆØ¯ÙŠØ©', 'Ø§Ù„Ø¥Ù…Ø§Ø±Ø§Øª']
        }
        country_csv = temp_workspace / "input_data" / "data_flat_file" / "dbo.CountryMaster.csv"
        self.create_csv_with_encoding(country_csv, country_data, 'utf-8')
        
        creator = IntegratedSearchFileCreator(str(temp_workspace))
        success = creator.create_integrated_file()
        
        assert success, "Should successfully parse Arabic UTF-8 CSV"
        
        # Verify Arabic characters are preserved
        company_names = creator.tables['CompanyMaster']['CompanyName'].tolist()
        assert 'Ø´Ø±ÙƒØ© Ø§Ù„ØªÙƒÙ†ÙˆÙ„ÙˆØ¬ÙŠØ§' in company_names
        assert 'Ù…Ø¤Ø³Ø³Ø© Ø§Ù„Ø¥Ù†ØªØ§Ø¬' in company_names
    
    def test_hindi_devanagari_utf8(self, temp_workspace, sample_relations):
        """Test parsing CSV with Hindi/Devanagari characters."""
        hindi_data = {
            'Id': [1, 2],
            'CompanyName': ['à¤ªà¥à¤°à¥Œà¤¦à¥à¤¯à¥‹à¤—à¤¿à¤•à¥€ à¤•à¤‚à¤ªà¤¨à¥€', 'à¤¨à¤¿à¤°à¥à¤®à¤¾à¤£ à¤‰à¤¦à¥à¤¯à¤®'],
            'Address': ['à¤¨à¤ˆ à¤¦à¤¿à¤²à¥à¤²à¥€, à¤­à¤¾à¤°à¤¤', 'à¤®à¥à¤‚à¤¬à¤ˆ, à¤­à¤¾à¤°à¤¤'],
            'CityName': ['à¤¨à¤ˆ à¤¦à¤¿à¤²à¥à¤²à¥€', 'à¤®à¥à¤‚à¤¬à¤ˆ'],
            'CompanyRegistrationDate': ['2020-01-15', '2019-05-20'],
            'Country_Fk_id': [11, 11]
        }
        
        company_csv = temp_workspace / "input_data" / "data_flat_file" / "dbo.CompanyMaster.csv"
        self.create_csv_with_encoding(company_csv, hindi_data, 'utf-8')
        
        country_data = {
            'Id': [11],
            'CountryName': ['à¤­à¤¾à¤°à¤¤']
        }
        country_csv = temp_workspace / "input_data" / "data_flat_file" / "dbo.CountryMaster.csv"
        self.create_csv_with_encoding(country_csv, country_data, 'utf-8')
        
        creator = IntegratedSearchFileCreator(str(temp_workspace))
        success = creator.create_integrated_file()
        
        assert success, "Should successfully parse Hindi UTF-8 CSV"
        
        # Verify Hindi characters are preserved
        company_names = creator.tables['CompanyMaster']['CompanyName'].tolist()
        assert 'à¤ªà¥à¤°à¥Œà¤¦à¥à¤¯à¥‹à¤—à¤¿à¤•à¥€ à¤•à¤‚à¤ªà¤¨à¥€' in company_names
        assert 'à¤¨à¤¿à¤°à¥à¤®à¤¾à¤£ à¤‰à¤¦à¥à¤¯à¤®' in company_names
    
    def test_mixed_encoding_files(self, temp_workspace, sample_relations):
        """Test handling multiple CSV files with different encodings."""
        # CompanyMaster in UTF-8 with Chinese
        chinese_data = {
            'Id': [1, 2],
            'CompanyName': ['åŒ—äº¬ç§‘æŠ€å…¬å¸', 'ä¸Šæµ·åˆ¶é€ '],
            'Address': ['åŒ—äº¬å¸‚', 'ä¸Šæµ·å¸‚'],
            'CityName': ['åŒ—äº¬', 'ä¸Šæµ·'],
            'CompanyRegistrationDate': ['2020-01-15', '2019-05-20'],
            'Country_Fk_id': [1, 1]
        }
        
        company_csv = temp_workspace / "input_data" / "data_flat_file" / "dbo.CompanyMaster.csv"
        self.create_csv_with_encoding(company_csv, chinese_data, 'utf-8')
        
        # CountryMaster in CP1252 with European characters
        country_data = {
            'Id': [1, 2],
            'CountryName': ['ä¸­å›½', 'FranÃ§ois']  # This will test fallback for problematic chars
        }
        country_csv = temp_workspace / "input_data" / "data_flat_file" / "dbo.CountryMaster.csv"
        try:
            self.create_csv_with_encoding(country_csv, country_data, 'cp1252')
        except UnicodeEncodeError:
            # Expected - create with UTF-8 instead
            self.create_csv_with_encoding(country_csv, country_data, 'utf-8')
        
        creator = IntegratedSearchFileCreator(str(temp_workspace))
        success = creator.create_integrated_file()
        
        assert success, "Should handle mixed encoding files gracefully"
        assert len(creator.tables) >= 2
    
    def test_special_characters_and_symbols(self, temp_workspace, sample_relations):
        """Test parsing CSV with special characters, symbols, and emojis."""
        special_data = {
            'Id': [1, 2, 3],
            'CompanyName': ['Techâ„¢ CorpÂ®', 'Data & AnalyticsÂ©', 'AI Solutionsâ„¢'],
            'Address': ['123 Main St. #456', 'Suite 789 @ Business Park', 'Floor 10, Tower-A'],
            'CityName': ['San JosÃ©', 'MontrÃ©al', 'SÃ£o Paulo'],
            'CompanyRegistrationDate': ['2020-01-15', '2019-05-20', '2021-03-10'],
            'Website': ['https://techâ„¢.com', 'www.data&analytics.org', 'ai-solutions.net'],
            'EmailId': ['info@techâ„¢.com', 'contact@data&analytics.org', 'hello@ai-solutions.net'],
            'Country_Fk_id': [1, 2, 3]
        }
        
        company_csv = temp_workspace / "input_data" / "data_flat_file" / "dbo.CompanyMaster.csv"
        self.create_csv_with_encoding(company_csv, special_data, 'utf-8')
        
        country_data = {
            'Id': [1, 2, 3],
            'CountryName': ['USA', 'Canada', 'Brazil']
        }
        country_csv = temp_workspace / "input_data" / "data_flat_file" / "dbo.CountryMaster.csv"
        self.create_csv_with_encoding(country_csv, country_data, 'utf-8')
        
        creator = IntegratedSearchFileCreator(str(temp_workspace))
        success = creator.create_integrated_file()
        
        assert success, "Should successfully parse CSV with special characters"
        
        # Verify special characters are preserved
        company_names = creator.tables['CompanyMaster']['CompanyName'].tolist()
        assert 'Techâ„¢ CorpÂ®' in company_names
        assert 'Data & AnalyticsÂ©' in company_names
    
    def test_corrupted_encoding_fallback(self, temp_workspace, sample_relations):
        """Test fallback mechanism for corrupted or problematic encodings."""
        # Create a file with intentionally mixed/corrupted encoding
        company_csv = temp_workspace / "input_data" / "data_flat_file" / "dbo.CompanyMaster.csv"
        
        # Manually write file with mixed encoding issues
        with open(company_csv, 'wb') as f:
            # Write header in UTF-8
            f.write('Id,CompanyName,Address,CityName,CompanyRegistrationDate,Country_Fk_id\n'.encode('utf-8'))
            # Write first row in UTF-8
            f.write('1,æ­£å¸¸å…¬å¸,åŒ—äº¬å¸‚,åŒ—äº¬,2020-01-15,1\n'.encode('utf-8'))
            # Write second row with Latin-1 bytes that aren't valid UTF-8
            f.write('2,'.encode('utf-8'))
            f.write('CafÃ© rÃ©sumÃ©'.encode('latin-1'))  # This creates invalid UTF-8 sequence
            f.write(',Paris,Paris,2019-05-20,2\n'.encode('utf-8'))
        
        country_data = {
            'Id': [1, 2],
            'CountryName': ['ä¸­å›½', 'France']
        }
        country_csv = temp_workspace / "input_data" / "data_flat_file" / "dbo.CountryMaster.csv"
        self.create_csv_with_encoding(country_csv, country_data, 'utf-8')
        
        creator = IntegratedSearchFileCreator(str(temp_workspace))
        success = creator.create_integrated_file()
        
        # Should succeed with fallback encoding
        assert success, "Should handle corrupted encoding with fallback mechanism"
        assert 'CompanyMaster' in creator.tables
    
    def test_empty_and_null_non_english_fields(self, temp_workspace, sample_relations):
        """Test handling of empty and null values in non-English text fields."""
        mixed_data = {
            'Id': [1, 2, 3, 4],
            'CompanyName': ['æ­£å¸¸å…¬å¸', '', 'ç©ºå€¼å…¬å¸', None],
            'Address': ['åŒ—äº¬å¸‚æœé˜³åŒº', None, '', 'ä¸Šæµ·å¸‚'],
            'CityName': ['åŒ—äº¬', '', None, 'ä¸Šæµ·'],
            'CompanyRegistrationDate': ['2020-01-15', '2019-05-20', '', '2021-03-10'],
            'Country_Fk_id': [1, 1, None, 1]
        }
        
        company_csv = temp_workspace / "input_data" / "data_flat_file" / "dbo.CompanyMaster.csv"
        
        # Create DataFrame and handle None values properly
        df = pd.DataFrame(mixed_data)
        df.to_csv(company_csv, index=False, encoding='utf-8')
        
        country_data = {
            'Id': [1],
            'CountryName': ['ä¸­å›½']
        }
        country_csv = temp_workspace / "input_data" / "data_flat_file" / "dbo.CountryMaster.csv"
        self.create_csv_with_encoding(country_csv, country_data, 'utf-8')
        
        creator = IntegratedSearchFileCreator(str(temp_workspace))
        success = creator.create_integrated_file()
        
        assert success, "Should handle empty/null values in non-English fields"
        
        # Verify the file was created and contains expected data
        output_file = temp_workspace / "processed_data_store" / "company_mapped_store" / "integrated_company_search.json"
        assert output_file.exists()
        
        with open(output_file, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        companies = data['companies']
        assert len(companies) == 4
        
        # Check that non-English characters are preserved where present
        company_names = [c['company_name'] for c in companies]
        assert 'æ­£å¸¸å…¬å¸' in company_names
        assert 'ç©ºå€¼å…¬å¸' in company_names
    
    def test_large_non_english_dataset(self, temp_workspace, sample_relations):
        """Test performance with larger dataset containing non-English characters."""
        # Create larger dataset with mixed international content
        large_data = {
            'Id': list(range(1, 101)),
            'CompanyName': [
                f'å…¬å¸{i}' if i % 4 == 0 else  # Chinese
                f'SociÃ©tÃ©{i}' if i % 4 == 1 else  # French
                f'ĞšĞ¾Ğ¼Ğ¿Ğ°Ğ½Ğ¸Ñ{i}' if i % 4 == 2 else  # Russian
                f'Company{i}'  # English
                for i in range(1, 101)
            ],
            'Address': [f'Address {i}' for i in range(1, 101)],
            'CityName': [f'City {i}' for i in range(1, 101)],
            'CompanyRegistrationDate': ['2020-01-15'] * 100,
            'Country_Fk_id': [1] * 100
        }
        
        company_csv = temp_workspace / "input_data" / "data_flat_file" / "dbo.CompanyMaster.csv"
        self.create_csv_with_encoding(company_csv, large_data, 'utf-8')
        
        country_data = {
            'Id': [1],
            'CountryName': ['International']
        }
        country_csv = temp_workspace / "input_data" / "data_flat_file" / "dbo.CountryMaster.csv"
        self.create_csv_with_encoding(country_csv, country_data, 'utf-8')
        
        creator = IntegratedSearchFileCreator(str(temp_workspace))
        success = creator.create_integrated_file()
        
        assert success, "Should handle large dataset with non-English characters"
        assert len(creator.tables['CompanyMaster']) == 100
        
        # Verify sampling of non-English characters
        company_names = creator.tables['CompanyMaster']['CompanyName'].tolist()
        assert any('å…¬å¸' in name for name in company_names)  # Chinese
        assert any('SociÃ©tÃ©' in name for name in company_names)  # French
        assert any('ĞšĞ¾Ğ¼Ğ¿Ğ°Ğ½Ğ¸Ñ' in name for name in company_names)  # Russian


def run_manual_test():
    """Manual test runner for development/debugging."""
    import tempfile
    import shutil
    
    print("ğŸš€ Running manual tests for non-English CSV parsing...")
    
    # Create temporary workspace
    temp_dir = tempfile.mkdtemp()
    workspace = Path(temp_dir)
    print(f"ğŸ“ Using temporary workspace: {workspace}")
    
    try:
        # Setup directory structure
        (workspace / "input_data" / "data_flat_file").mkdir(parents=True)
        (workspace / "input_data" / "table_relations").mkdir(parents=True)
        
        # Create relations.json
        relations = [
            {
                "from_table": "CompanyMaster",
                "from_column": "Country_Fk_id",
                "to_table": "CountryMaster",
                "to_column": "Id"
            }
        ]
        
        relations_path = workspace / "input_data" / "table_relations" / "relations.json"
        with open(relations_path, 'w', encoding='utf-8') as f:
            json.dump(relations, f, ensure_ascii=False)
        
        # Test 1: Chinese characters
        print("\nğŸ‡¨ğŸ‡³ Testing Chinese characters...")
        chinese_data = {
            'Id': [1, 2],
            'CompanyName': ['åŒ—äº¬ç§‘æŠ€æœ‰é™å…¬å¸', 'ä¸Šæµ·åˆ¶é€ ä¼ä¸š'],
            'Address': ['åŒ—äº¬å¸‚æœé˜³åŒº', 'ä¸Šæµ·å¸‚æµ¦ä¸œæ–°åŒº'],
            'CityName': ['åŒ—äº¬', 'ä¸Šæµ·'],
            'CompanyRegistrationDate': ['2020-01-15', '2019-05-20'],
            'Country_Fk_id': [1, 1]
        }
        
        company_csv = workspace / "input_data" / "data_flat_file" / "dbo.CompanyMaster.csv"
        pd.DataFrame(chinese_data).to_csv(company_csv, index=False, encoding='utf-8')
        
        country_data = {
            'Id': [1],
            'CountryName': ['ä¸­å›½']
        }
        country_csv = workspace / "input_data" / "data_flat_file" / "dbo.CountryMaster.csv"
        pd.DataFrame(country_data).to_csv(country_csv, index=False, encoding='utf-8')
        
        # Test the parser
        creator = IntegratedSearchFileCreator(str(workspace))
        success = creator.create_integrated_file()
        
        if success:
            print("âœ… Chinese characters test passed!")
            
            # Check output
            output_file = workspace / "processed_data_store" / "company_mapped_store" / "integrated_company_search.json"
            if output_file.exists():
                with open(output_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                
                print(f"ğŸ“Š Created file with {len(data['companies'])} companies")
                sample_company = data['companies'][0]
                print(f"ğŸ“ Sample company: {sample_company['company_name']}")
                print(f"ğŸ¢ Company country: {sample_company.get('country', 'N/A')}")
        else:
            print("âŒ Chinese characters test failed!")
        
        print("\nğŸ‰ Manual test completed!")
        
    except Exception as e:
        print(f"âŒ Manual test failed with error: {e}")
        import traceback
        traceback.print_exc()
    finally:
        # Cleanup
        shutil.rmtree(temp_dir)
        print(f"ğŸ§¹ Cleaned up temporary workspace")


if __name__ == "__main__":
    # Run manual test if script is executed directly
    run_manual_test()
